===============================================================================================
Layer (type:depth-idx)                        Output Shape              Param #
===============================================================================================
ViT                                           [75, 3]                   --
├─Embedding: 1-1                              [75, 17, 768]             1,036,800
│    └─Conv2d: 2-1                            [75, 768, 4, 4]           590,592
│    └─Flatten: 2-2                           [75, 768, 16]             --
├─Dropout: 1-2                                [75, 17, 768]             --
├─Sequential: 1-3                             [75, 17, 768]             --
│    └─TransformerEncoderBlock: 2-3           [75, 17, 768]             --
│    │    └─MultiHeadAttentionBlock: 3-1      [75, 17, 768]             2,363,904
│    │    └─MLPBlock: 3-2                     [75, 17, 768]             4,723,968
│    └─TransformerEncoderBlock: 2-4           [75, 17, 768]             --
│    │    └─MultiHeadAttentionBlock: 3-3      [75, 17, 768]             2,363,904
│    │    └─MLPBlock: 3-4                     [75, 17, 768]             4,723,968
│    └─TransformerEncoderBlock: 2-5           [75, 17, 768]             --
│    │    └─MultiHeadAttentionBlock: 3-5      [75, 17, 768]             2,363,904
│    │    └─MLPBlock: 3-6                     [75, 17, 768]             4,723,968
│    └─TransformerEncoderBlock: 2-6           [75, 17, 768]             --
│    │    └─MultiHeadAttentionBlock: 3-7      [75, 17, 768]             2,363,904
│    │    └─MLPBlock: 3-8                     [75, 17, 768]             4,723,968
│    └─TransformerEncoderBlock: 2-7           [75, 17, 768]             --
│    │    └─MultiHeadAttentionBlock: 3-9      [75, 17, 768]             2,363,904
│    │    └─MLPBlock: 3-10                    [75, 17, 768]             4,723,968
│    └─TransformerEncoderBlock: 2-8           [75, 17, 768]             --
│    │    └─MultiHeadAttentionBlock: 3-11     [75, 17, 768]             2,363,904
│    │    └─MLPBlock: 3-12                    [75, 17, 768]             4,723,968
│    └─TransformerEncoderBlock: 2-9           [75, 17, 768]             --
│    │    └─MultiHeadAttentionBlock: 3-13     [75, 17, 768]             2,363,904
│    │    └─MLPBlock: 3-14                    [75, 17, 768]             4,723,968
│    └─TransformerEncoderBlock: 2-10          [75, 17, 768]             --
│    │    └─MultiHeadAttentionBlock: 3-15     [75, 17, 768]             2,363,904
│    │    └─MLPBlock: 3-16                    [75, 17, 768]             4,723,968
├─Sequential: 1-4                             [75, 3]                   --
│    └─Linear: 2-11                           [75, 768]                 590,592
│    └─LayerNorm: 2-12                        [75, 768]                 1,536
│    └─ReLU: 2-13                             [75, 768]                 --
│    └─Linear: 2-14                           [75, 3]                   2,307
===============================================================================================
Total params: 58,924,803
Trainable params: 58,924,803
Non-trainable params: 0
Total mult-adds (G): 3.59
===============================================================================================
Input size (MB): 3.69
Forward/backward pass size (MB): 446.98
Params size (MB): 155.96
Estimated Total Size (MB): 606.62
===============================================================================================